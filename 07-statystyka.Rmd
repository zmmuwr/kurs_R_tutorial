# Statystyka

W tej części opieram się w dużej mierze na książce "Przewodnik po pakiecie R" P. Biecka oraz na kursie "Środowisko R od podstaw". Nie ma ona być wprowadzeniem do statystyki jako takiej, ale jedynie pokazaniem jakie funkcje R można wykorzystać do przeprowadzenie często stosowanych analiz statystycznych.

Przystępne wprowadzenie do statystyki dla biologów znalazłam na stronie dotyczącej badania _C. elegans_ - [wormbook.org](http://www.wormbook.org/chapters/www_statisticalanalysis/statisticalanalysis.html#sec1). Bardzo mało matematyki i wzorów, za to sporo przykładów, szkoda tylko, że nie ma nic o R ;)

## Podstawowe statystyki opisowe

Skrócone podsumowanie danych liczbowych otrzymamy przy użyciu `summary` - średnia, mediana, wartość min i max, oraz pierwszy i trzeci kwantyl. 

Podstawowe funkcje służące do opisu danych to:

Funkcja | Opis | Uwagi
----|----|-----
`mean`  | średnia |  
`median` | mediana |  
`sd` | odchylenie standardowe |  
`var` | wariancja |  
`min` | wartość minimalna |  
`max` | wartość maksymalna |  
`range` | zakres danych |  
`IQR` | rozstęp kwartylowy |  
`geometric.mean` | średnia geometryczna |  
`weighted.mean` | średnia ważona | musimy podać wektor wag do każdego elementu
`kurtosis` | kurtoza | pakiet moments
`skewness` | skośność | pakiet moments
`mlv` | moda | pakiet modeest |  
`quantile` | wybrane kwantyle | należy podać, które kwantyle mają być policzone
`mad` | odchylenie medianowe |  

```{r}

wektor <- rnorm(1000, mean=2)

mean(wektor)
median(wektor)
range(wektor)

quantile(wektor, c(0.25, 0.4, 0.5, 0.6, 0.75))

library(modeest)
mlv(wektor, method = 'shorth')

```

## Liczby pseudolosowe

Podczas pracy w R często przydaje się możliwość szybkiego wygenerowania liczb z danego rozkładu. Można w ten sposób np. przetestować nową funkcję (również własną ;) )

Wszystkie takie funkcje zaczynają się od r, a ich pierwszy argument to ilość liczb jaka ma zostać wygenerowana.

Jeżeli chcemy dwa razy wygenerować takie same liczby należy najpierw ustawić ziarno - `set.seed`

Funkcja | opis | parametry
---|---|---
`runif` | rozkład jednostajny, od 0 do 1 | zmiana wartości `min` i `max` rozkładu
`rnorm` | rozkład normalny, średnia = 0, odchylenie = 1 | zmiana `mean` i `sd`
`rlnorm` | rozkład log-normalny | zmiana `meanlog` i `sdlog`
`rexp` | rozkład wykładniczy | zmiana `rate`
`rbinom` | rozkład dwumianowy | ustawiamy wielkość (`size`) i prawdopodobieństwo (`prob`)

```{r}
a <- rnorm(1000)
b <- rlnorm(1000)
c <- rexp(1000)
d <- runif(1000)

par(mfrow=c(2,2))
hist(a, main="Normalny")
hist(b, main = "Log-Normalny")
hist(c, main = "Wykładniczy")
hist(d, main = "Jenostajny")

par(mfrow=c(1,1))

```

Możemy też potrzebować wektor losowych wartości z danego zakresu. Służy do tego funckja `sample`. Pierwszym jej argumentem jest wektor, z którego mają być losowane wartości, drugim ilość elementów do losowania. Można też podać czy losowanie ma być ze zwracaniem (`replace=TRUE`) i wektor prawdopodobieństwa dla każdego elementu, jeżeli nie mają być takie same. Prawdopodobieństwa muszą sumować się do 1.

```{r}
# Symulacja 200 rzutów kością

kosc <- sample(1:6, 200, replace = TRUE)
barplot(table(kosc))

# Symulacja nieuczciwej kości

kosc2 <- sample(1:6, 200, replace = TRUE, prob = c(0.1, 0.1, 0.1, 0.2, 0.2, 0.3))
barplot(table(kosc2))
```


## Podstawowe testy statystyczne

### Test t-studenta - weryfikacja równości średnich

Można go wykonać dla jednej lub dwóch prób przy pomocy funkcji `t.test`. Dane muszą być z rozkładu normalnego, ale domyślnie nie muszą mieć równej wariancji.

Przy teście dla jednej próby należy podać wartość średniej `mu`, do której ma zostać przyrównana próba (domyślnie wynosi 0).

Przy teście dla dwóch prób możemy również wykonać test dla prób sparowanych - `paired=TRUE` - sprawdzamy czy różnica między próbami jest różna od 0 np. dane przed i po dodaniu jakiegoś czynnika

Domyślnie wykonywany jest test dwustronny, możemy to zmienić parametrem `alternative` ustawiając `"less"` albo `"greater"`.

Wynik testu podaje nam kilka wartości:

* wartość statystyki testowej

* ilość stopni swobody

* p-value - minimalny poziom istotności dla którego możemy odrzucić hipotezę zerową. Np. p-value równy 0.05 oznacza, że jeżeli odrzucimy hipotezę zerową istnieje 5% szans, że popełnimy błąd

* hipoteza alternatywna

* przedział ufności dla wyliczonej średniej albo różnicy między średnimi

* średnia z próby

Możemy wyświetlić wszystkie te wartości albo jedynie interesujące nas poprzez znak $ np. `t.test(x)$p.value`. Wynik takiego testu można też przypisać do zmiennej i wykorzystać później.

```{r}

x <- rnorm(100)

y <- rnorm(100, mean=1)

# test dla jednej próby, porównanie do średnia równej 0
t.test(x)

t.test(y)

# test dla dwóch prób
t.test(x,y)

wynik <- t.test(x,y)
wynik$p.value
wynik$conf.int

# test t można przeprowadzić dla większej ilości grup przy pomocy pairwise.t.test, 
# wartości p zostaną wtedy dostoswane do wielokrotnego powtarzania testu
pairwise.t.test(dane1$pomiar, dane1$Szczep)

```

Jeżeli nie wiemy z jakiego rozkładu pochodzą dane możemy wykorzystać test nieparametryczny - test Wilcoxona `wilcox.test`, też podaje wartość p.

Analogicznie dla `t.test` istnieje funkcja `pairwise.wilcox.test`

```{r}

wilcox.test(x,y)

```


### Test F - weryfikacja równości wariancji

Podobny w składni do `t.test`, wykonujemy funkcją `var.test`. Również zakładamy, że dane pochodzą z rozkładu normalnego. Zastosowanie go dla danych z innych rozkładów może prowadzić do błędnych wniosków.

```{r}

var.test(x,y)

z <- rnorm(100, sd=3)

var.test(x,z)
```

### Testowanie zgodności z rozkładem normalnym i dopasowywanie parametrów rozkładu

Metoda graficzna to wspomniany już wykres kwantylowy (`qqPlot`).

Kilka testów, które można wykorzystać znajduje się w pakiecie nortest np. `shapiro.test`, `cvm.test`. 

Test Shapiro-Wilka należy do bardziej popularnych, liczba obserwacji powinna mieścić się w zakresie od 3 do 5000.

```{r}

x <- rnorm(500)
y <- rlnorm(500, sdlog = 0.5)

# histogram i wykres kwantylowy dla danych x i y
par(mfrow = c(2,2))
hist(x)
hist(y)
qqPlot(x)
qqPlot(y)

par(mfrow = c(1,1))

# test Shapiro-Wilka do sprawdzania zgodności z rozkładem normalnym
shapiro.test(x)
shapiro.test(y)

```

Do sprawdzenie zgodności z zadanym rozkładem możemy wykorzystać test Kołmogorowa-Smirnowa - `ks.test`. Do funkcji należy podać wektor obserwacji oraz albo drugi wektor obserwacji (sparwdzamy czy pochodzą z takiego samego rozkładu) albo nazwę funkcji obliczającej dystrybuantę rozkładu np. `pnorm` (normalny), `plnorm` (log-normalny), `punif` (jednostajny).

Jeżeli parametry rozkładu różnią się od domyślnych to należy je podać np. `mean` i `sd` dla rozkładu normalnego.

```{r}
x <- rnorm(1000)

x1 <- rnorm(1000)

x2 <- rnorm(1000, mean = 2)

y <- rlnorm(1000, sdlog = 0.5)

# Sprawdzamy czy obserwacje mają taki sam rozkład
ks.test(x,x1)
ks.test(x,x2)
ks.test(x,y)

# Sprawdzamy czy x pochodzi z rozkładu normalnego czy log-normalnego
ks.test(x, pnorm)
ks.test(x, plnorm)

# Jeżeli nie podamy, że średnia x2 równa się 2 otrzymamy błędny wynik
ks.test(x2, pnorm)
ks.test(x2, pnorm, mean = 2)

# Czy y pasuje do rozkładu normalnego czy log-normalnego
ks.test(y, pnorm, sd = 0.5)
ks.test(y, plnorm, sd = 0.5)

```

Do oszacowania parametrów rozkładu można wykorzystać funkcję `fitdistr` z pakietu MASS. Podajemy wektor obserwacji oraz funkcję gęstości rozkładu. Rozpoznawane funkcje to np. `"normal", "log-normal", "exponential", "f", "t"` itp. 

```{r}
x <- rnorm(1000, mean=20, sd=0.5)

library(MASS)

# obliczamy parametry rozkładu normalnego i log-normalnego dla danych x
fit <- fitdistr(x, "normal")
fit
fit2 <- fitdistr(x, "log-normal")
fit2

# sprawdzamy czy dane x faktycznie pochodzą z rozkładu o parametrach obliczonych wyżej
ks.test(x, pnorm, fit$estimate[1], fit$estimate[2])
ks.test(x, pnorm, fit2$estimate[1], fit2$estimate[2])

y <- rlnorm(1000, sdlog = 0.4)
fitdistr(y, "log-normal")
```


### Obliczanie przedziałów ufności dla średniej i błędu standardowego

Błąd standardowy obliczamy ze wzoru: $$SEM= odch/\sqrt{n}$$. Pokazuje estymowane odchylenie pomiędzy prawdziwą średnią populacji, a obliczoną dla próby. 

W R nie ma funkcji liczącej błąd standardowy, ale można takie przeliczenie wykonać samemu albo napisać taką funkcję

```{r}
x <- rnorm(300, mean=10, sd=0.5)

mean(x)

sd(x)

SEM <- sd(x)/sqrt(length(x))

cat("Błąd standardowy wynosi", SEM)
```

Przedział ufności o istotności 0.95 mówi nam, że prawdopodobieństwo znalezienia estymowanego parametru w tym przedziale wynosi 95%.

Przedział ufności dla rozkładu normalnego przy poziomie istotności 0.95 wyliczamy ze wzoru:
$$ci= 1.96*odch/\sqrt{n}$$ 
$$Średnia +/- ci$$

1.96 to 0.975 kwantyl rozkładu normalnego.

Zamiast 1.96 możemy podstawić wartość wyliczoną z rozkładu t przy pomocy: `qt(0.975, df)`

Przy dużej liczebności próby (ok. 300) wyjdzie na to samo.

```{r}
set.seed(120)
x <- rnorm(100, mean=3, sd=0.75)

(srednia<-round(mean(x),2))
round(sd(x),2)

ci <- round(1.96*sd(x)/sqrt(length(x)),2)

lower <- srednia-ci
upper <- srednia+ci

cat("Średnia x wynosi", srednia, "w przedziale ufności", lower, upper)

ci <- round(qt(0.975, df=(length(x)-1))*sd(x)/sqrt(length(x)),2)

lower <- srednia-ci
upper <- srednia+ci

cat("Średnia x wynosi", srednia, "w przedziale ufności", lower, upper)

```

Przedział ufności dla średniej można też znaleźć w wyniku `t.test`.

### Testowanie korelacji

Wartość korelacji zawiera się pomiędzy -1 a 1. 0 oznacza całkowity brak korelacji. Do 0.7 korelację określamy jako silną. 

Korelację obserwacji z dwóch wektorów albo całej macierzy można obliczyć przy pomocy funkcji `cor`. Domyślnie obliczona zostanie za pomocą metody Pearsona, można to zmienić na metodę Spearmana - `method="spearman"`. Metoda Spearmana jest mniej wrażliwa na obserwacje odstające.

Jeżeli chcemy poznać szczegóły dotyczące korelacji możemy użyć `cor.test`. Też można wybrać metodę, podaje również wartość p dla korelacji, przedział ufności, ilość stopni swobodi itp.

```{r}
x <- sort(runif(30))
y <- sort(runif(30))

plot(x, y)

cor(x, y)
cor(x, y, method = "spearman")
cor.test(x, y)

# dodajemy wartość bardzo odstającą :)
x <- c(x, -100)
y <- c(y, 10)

plot(x, y)

# wartość korelacji wyliczona metodą spearmana zmienia się dużo mniej
cor(x, y)
cor(x, y, method = "spearman")

# wartość odstająca może też dać korelację pearsona tam gdzie jej wcale nie ma
# ale korelacja spearmana nie zmieni się tak bardzo

x <- sort(runif(30))
y <- runif(30)
plot(x, y)
cor(x, y)
x <- c(x, 10)
y <- c(y, 10)
plot(x, y)
cor(x, y)
cor(x, y, method="spearman")

```


### Test chi-kwadrat - zgodność rozkładu zmiennych jakościowych

Służy do weryfikacji zależności pomiędzy dwiema zmiennymi jakościowymi, funkcja `chisq.test`. Jako argument najlepiej podstawić macierz kontyngencji wyliczoną funkcją `table`. Poza wartością p można też sprawdzić wartości oczekiwane - `wynik$expected`. Test chi^2 można wykorzystać też dla wiekszych macierzy.

Dla tablic 2x2 można również użyć dokładnego testu Fishera - `fisher.test`.

```{r}
# Wygenerujemy przykładowe dane - takie same

x <- data.frame(jeden  =  sample(c("A","B"),200, replace = TRUE), 
                dwa  =  sample(c("grupa1","grupa2"),200, replace = TRUE))

# różne

y <- data.frame(jeden  =  c(sample(c("A","B"),100, replace = TRUE, prob = c(0.4,0.6)),
                            sample(c("A","B"),100, replace = TRUE, prob = c(0.7,0.3))),
                dwa  =  rep(c("grupa1","grupa2"),each = 100))

(tabela_x <- table(x))

(tabela_y <- table(y))

chisq.test(tabela_x)
chisq.test(tabela_y)

fisher.test(tabela_x)
fisher.test(tabela_y)
```

Jeżeli chcemy tylko sprawdzić prawdopodobieństwo możemy użyć testu propocji - `prop.test`

```{r}
# przykładowo czy 785 sukcesów na 1500 prób jest istotnie różne od p=0.5

prop.test(785, 1500, p = 0.5)

# można też wykonać test proporcji dla większej ilości danych - pairwise.prop.test

x <- c(190,475,350,65)
n <- c(500,1000,800,250)

pairwise.prop.test(x, n, p.adjust.method = "bonferroni")

```


## Modelowanie - czyli jak dopasować linię trendu

Zależność pomiędzy dwiema zmiennymi ilościowymi najłatwiej przedstawić na wykresie rozrzutu. Kolejnym krokiem może być próba dopasowania do danych jakiegoś modelu np. liniowego, ale może być też bardziej złożony np. model wzrostu logistycznego, Michaelisa-Menten itp. 

### Regresja liniowa

Najbardziej popularną funkcją w R do wyznaczania modeli liniowych jest `lm`. Wymaga jedynie podania formuły opisującej modelowaną przez nas zależność i ramki danych.

```{r}

# Jako przykład wykorzystamy zbiór danych R dotyczący wzrostu drzewek pomarańczowych

orange <- Orange

head(orange, 3)

# wykres obwodu drzewa od wieku

plot(orange$age, orange$circumference)

```

Formuły zapisujemy korzystając z ~ , np.: 
* zależność y od x: y~x, 
* zależność y od x i z: y ~ x + z, 
* zależność y od x, z i interakcji pomiędzy x i z: y ~ x + z + x:z albo y ~ x * z

```{r}

fit <- lm(circumference~age, data = orange)

fit$coefficients

summary(fit)

plot(orange$age, orange$circumference)
abline(fit, col = "green3")
```

Jeżeli chcemy jedynie wyznaczyć równanie funkcji liniowej - y = ax + b, wystarczy sprawdzić współczynniki dopasowanego modelu - coefficients. Intercept oznacza miejsce przecięcia z osią Y - współczynnik b, a age to wartość przez którą należy pomnożyć wiek drzewa żeby uzyskać jego obwód.

Można powiedzić że według naszego modelu każdego roku obwód drzewa zwiększa się o wartość 0.107 mm.

Funkcja `summary` pozwala zobaczyć wszystkie istotne informacje dotyczące naszego modelu.

Pierwsza linijka podsumowania zawiera powtórzenie formuły jaką podaliśmy w funkcji.

Następnie mamy podsumowanie wartości residuals - reszt. Są to różnice pomiedzy faktycznymi wartościami a wyznaczonymi ze wzoru. Model jest dobierany tak żeby suma kwadratów reszt była jak najmniejsza.

Potem mamy tabelę z wymienionymi wszystkimi współczynnikami modelu. Istotna jest ostatnia kolumna zawierająca wartość p, wartość powyżej 0.05 sugeruje że dana zmienna nie jest istotna dla modelu i można ją z niego usunąć.

Na końcu otrzymujemy jeszcze kilka wartości oceniających dobrany model jako całość. Znajoma powinna być wartość R-squared. Przyjmuje ona wartości z zakresu od 0 do 1 i oznacza procent wariancji obecnej w danych jaka może być wyjaśniona przy pomocy danego modelu. Może być przydatna przy ocenie który z kilku modeli wybrać do opisu naszych danych. Wartość adjusted R squared uwzględnia również ilość zmiennych jakie podaliśmy w formule.

### Zmienne jakościowe w modelu

W modelu możemy uwzględniać również zmienne jakościowe, muszą one jednak zostać zakodowane - zmienione w dane liczbowe. Funkcja `lm` domyślnie uznaje pierwszy poziom zmiennej za referencyjny, kolejne poziomy bedą się do niego odnosić.

```{r}

# zbiór danych dotyczący wzrostu kurzcaków w zależności od rodzaju karmy

chick <- ChickWeight

summary(lm(weight ~ Time + Diet, data = chick))

p <- ggplot(chick, aes(x = Time, y = weight))

p + geom_point() + stat_smooth(aes(color = Diet), method = "lm", se = FALSE, size = 1)
```

Wartości przy przy kolejnych typach karmy - Diet2, Diet3, Diet4, pokazują o ile średnio różnią się wartości wyznaczone dla nich od poziomu referencyjnego czyli Diet1. To znaczy, że kurczaki karmione karmą nr 2 są o 16.17 g cięższe od kurczaków karmionych karmą nr 1.


### Pakiet drc - dose response models

Pakiet drc zawiera szereg funkcji ułatwiających dopasowywanie do danych wiele modeli popularnych w biologii np. wzrost logistyczny, rozpad eksponencjalny, Michaelisa-Menten i inne.

Podstawową funkcja to `drm`, w której podajemmy formułę, podobnie jak w funkcji `lm` oraz koniecznie funkcję startową, która ma zostać użyta do poszukiwania modelu. Funkcje o kolejnych numerach różnią się ilością parametrów, które są uznawane za stałe np. MM.2 zakłada że przy x = 0, y też jest równe 0.

Model | Funkcja startowa
-------|-------
Logistyczny | LL.2, LL.3, LL.4, LL.5
Michaelisa-Menten | MM.2, MM.3
Eksponencjalny | EXD.2, EXD.3

Wykres razem z dopasowaniem można uzyskać dzięki standardowej funkcji `plot`. Można również porównać kilka modeli podając wektor ze zmienną dzielącą dane na poziomy.

```{r}
library(drc)

# przykładowe dane

dane <- data.frame(stezenie  = c( 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 1, 1.5), 
                   predkosc = c( 0.3, 0.5, 0.8, 0.9, 1, 1.02, 1.09, 1.15, 1.23))

model <- drm(predkosc~stezenie, data = dane, fct = MM.2())

# d to prędkośc maksymalna, e to Km
# w podsumowaniu dostajemy błąd standardowy i wartość p
summary(model)

plot(model, log="")

# przykładowe dane do porównania np. dwóch enzymów
dane1 <- dane
dane1$predkosc <- dane1$predkosc/2
dane <- rbind(dane, dane1)
dane$poziom <- rep(c("A", "B"), each = 9)

# model z dodanym podziałem na poziomy, możemy też nazwać oznaczane parametry
model <- drm(predkosc~stezenie, poziom, data = dane, fct = MM.2(names = c(d = "Vmax", e = "Km")))

summary(model)

plot(model, log="")

# Podobnie wygląda dopasowywanie funkcji logistycznej
# Korzystamy z przykładowych danych z pakietu drc dla wpływu herbicydów na tylakoidy

spinach.m1 <- drm(SLOPE~DOSE, CURVE, data = spinach, fct = LL.4())

summary(spinach.m1)

plot(spinach.m1)
```

Możliwości tego pakietu sa dużo większe niż przedstawione powyżej. Autorzy przygotowali bardzo dobrą instrukcję użytkowania swojego pakietu razem ze szczegółowymi przykładami użycia wszystkich funkcji. Można ją znaleźć na stronie [Biossay](http://www.bioassay.dk/).

## Analiza ANOVA

Test ANOVA zakłada, że nasze dane pochodzą z rozkładu normalnego, mają takie same wariancje i są niezależne.

Jeżeli nasze dane nie spełniają wymogu normalności można albo dane znormalizować (np logarytm) albo zastosować test nieparametryczny np. test Kruskala_Wallisa albo test Friedmana. 

Analizę wariancji można podzielić na jedno- i wieloczynnikową. Dane powinny zawierać wektor wartości (ilościowy), które można pogrupować wobec jednej lub więcej zmiennych jakościowych. 

ANOVA w R można przeprowadzić na kilka sposobów, do popularnych należą funkcje `anova` i `aov`, które różnią się sposobem wywołania i prezentacji wyników.

Aby stwierdzić które średnie w naszym zbiorze danych różnią się można przeprowadzić tzw. testy post hoc np. test HSD Tukeya - `TukeyHSD`, `HSD.test` (pakiet agricolae).

```{r}
# przykładowe dane z rozkładu normalnego, różniące się średnią

dane <- data.frame(x = c(rnorm(200,1), rnorm(200,1.3), rnorm(200, 1)), 
                   y = rep(c("A","B","C"), each = 200))

# ANOVA przy pomocy funkcji aov, konieczne użycie summary dla wyświetlenia wyniku

model2 <- aov(x~y, data = dane)
summary(model2)

# test Tukeya
# Otrzymujemy wartość p dla każdej pary czynników i różnicę z przedziałem ufności

TukeyHSD(model2)

# wynik testu Tukeya można pokazać na wykresie

plot(TukeyHSD(model2))

# Analiza dwuczynnikowa
# Dodajemy nową kolumnę do danych zawierającą losowy czynnik z - nie powinien wpływać na średnią
dane <- data.frame(dane, z = sample(c("d", "e", "f"), 600, replace = T))

model3 <- aov(lm(x~y + z, data = dane))
summary(model3)

TukeyHSD(model3)

# z analizą interakcji między y i z

model4 <- aov(lm(x~y + z + y:z, data = dane))
summary(model4)

TukeyHSD(model4)

# test nieparametryczny Kruskala-Wallisa z pakietu podstawowego
wynik <- kruskal.test(dane$x,dane$y)
wynik

# albo z pakietu agricolae - pokazuje nie tylko czy coś się różni, ale również które grupy

wynik <- agricolae::kruskal(dane$x,dane$y)
wynik


```


# Miscellaneous

* Pakiet beepr pozwala na generowanie dźwięków - funkcja `beep` :) Dostępnych jest 10 różnych, można również podać własny plik .wav. Może się przydać jeżeli uruchamiamy w R jakiś dłuższy proces i chcemy wiedzieć kiedy się skończy.

* Pakiet fortunes zawiera zbiór cytatów na temat R, funkcja `fortune`

* Cytowanie R - dla całego R - wpisujemy `citation()`, dla poszczególnych pakietów `citation("nazwa_pakietu")`, podaje też cytowanie w formacie latex.

```{r}
citation()

citation('ggplot2')
```
